{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca1a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='New Delhi\\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=3) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of India?\", source=\"user\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc123750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=api_key\n",
    ")\n",
    "message = [UserMessage(content=\"what is the capital of india\", source=\"user\")]\n",
    "response = await model_client.create(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b78e4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateResult(finish_reason='stop', content='The capital of India is **New Delhi**.\\n', usage=RequestUsage(prompt_tokens=6, completion_tokens=10), cached=False, logprobs=None, thought=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6aa6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0\n",
    ")\n",
    "response=llm.invoke(\"what is length of wall of china?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfdbd6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, so I need to figure out the length of the Great Wall of China. I remember hearing that it's really long, but I'm not exactly sure how long. I think it's somewhere in the thousands of kilometers, but I'm not certain. Let me try to break this down.\\n\\nFirst, I know the Great Wall is a series of fortifications built across several Chinese dynasties. It wasn't built all at once, so maybe that's why the length can vary. I've heard different numbers before, like 13,000 miles or something like that, but I'm not sure if that's accurate.\\n\\nI should consider the different dynasties and how much each contributed to the wall. The Qin Dynasty, I think, was the first to build a long version of the wall, but it was probably not as extensive as later versions. Then the Han Dynasty might have extended it further. The most famous parts were probably built during the Ming Dynasty, which was much later.\\n\\nI also remember that the wall isn't a single continuous structure. There are sections that are broken, and some parts have been destroyed over time. So, when measuring the length, do they include all the separate sections, or just the main ones? That could affect the total length.\\n\\nI think in recent years, there was a survey done using more accurate methods like GPS and satellites. Maybe that survey gave a more precise measurement than the older estimates. I believe the number was around 13,170 miles, but I'm not 100% sure. I should check if that's the most recent figure or if there's a newer one.\\n\\nAlso, the wall's condition varies a lot. Some parts are well-preserved and touristy, while others are overgrown or in ruins. I wonder if the length includes all sections, regardless of their condition, or only the parts that are still standing.\\n\\nAnother thing to consider is that the wall was built for different purposes in different areas. In some places, it's more about defense, and in others, it might have been used for controlling trade routes. This could mean that the wall's construction and length varied based on strategic needs.\\n\\nI'm also thinking about how the wall is measured. Is it just the outer edges, or do they include all the watchtowers and beacon towers? Those structures might add to the overall length, but I'm not sure if they're included in the measurements.\\n\\nWait, I think the 13,170-mile figure includes all the branches and spurs, not just the main wall. That makes sense because the wall isn't a straight line; it has many turns and branches. So, adding all those up would give a longer total length.\\n\\nI should also consider the units. Sometimes people use kilometers instead of miles, so I need to make sure I'm consistent. 13,170 miles converts to about 21,200 kilometers. That seems really long, but considering the wall spans across mountains and valleys, it might add up.\\n\\nI'm trying to remember if there's any controversy or debate about the exact length. I think some sources might underestimate or overestimate based on how they define the wall's boundaries. For example, some might only count the parts built during a specific dynasty, while others include all historical versions.\\n\\nIn summary, I think the most commonly cited length is around 13,170 miles or 21,196 kilometers, which includes all the sections, branches, and spurs. This figure comes from a more recent and thorough survey, so it's probably the most accurate estimate we have now.\\n</think>\\n\\nThe Great Wall of China is estimated to be approximately 13,170 miles (21,196 kilometers) in length. This measurement includes all the various sections, branches, and spurs, and is based on a recent, comprehensive survey that utilized modern techniques like GPS. This figure accounts for the wall's construction across multiple dynasties and its strategic purposes, which led to its complex and extensive layout.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 811, 'prompt_tokens': 11, 'total_tokens': 822, 'completion_time': 3.40752786, 'prompt_time': 0.000319268, 'queue_time': 0.053824232, 'total_time': 3.4078471280000002}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'finish_reason': 'stop', 'logprobs': None}, id='run--31a57631-b57e-4c43-80b5-139cb67e845b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 811, 'total_tokens': 822})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb4883",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatGroq' object has no attribute 'model_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautogen_agentchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssistantAgent\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m agent = \u001b[43mAssistantAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msample_agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are an helpful assistent that can tasked with answering user queries.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muham\\Desktop\\Agentic_ai_class\\venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:773\u001b[39m, in \u001b[36mAssistantAgent.__init__\u001b[39m\u001b[34m(self, name, model_client, tools, workbench, handoffs, model_context, description, system_message, model_client_stream, reflect_on_tool_use, max_tool_iterations, tool_call_summary_format, tool_call_summary_formatter, output_content_type, output_content_type_format, memory, metadata)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28mself\u001b[39m._tools: List[BaseTool[Any, Any]] = []\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_info\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mfunction_calling\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe model does not support function calling.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    775\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\main.py:989\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ChatGroq' object has no attribute 'model_info'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "agent = AssistantAgent(\n",
    "    name=\"sample_agent\",\n",
    "    model_client=llm,\n",
    "    system_message=\"You are an helpful assistent that can tasked with answering user queries.\",\n",
    "    tools=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d7d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
